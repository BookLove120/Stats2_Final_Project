---
output:
  pdf_document: default
  html_document: default
---
# Instructions
You're almost done with the semester! Take a second to congratulate yourself on getting here. As a reminder, this final project is simply an (imperfect) way of measuring what you have learned throughout the semester. So take a deep breath and do your best, but also remember that it doesn't determine your value as a human being.

The exam is split into 4 sections: Module 1, 2 and 3 (6 questions), Modules 4 and 5 (3 questions), Module 6 (2 questions) and the final project. Most of the questions on this exam are short answers. You don't need to write out an overly long response (a sentence or so for each part of the question should be fine), but you should be specific in explaining your response. For example, if there is a question about whether the assumptions are reasonable. You shouldn't just say "from the plot we can see that the linearity assumption is (or is not) reasonable," but instead you should explain specifically why the plot leads you to believe the linearity assumption is (or is not) reasonable.

The exam is open notes so you **can** use any of the material or any of the notes you have taken throughout the class. You **cannot** discuss the exam (while it is in progress) with anyone else. You also **cannot** use any generative AI tools. Submissions will be sent by e-mail to **nbb45@cornell.edu** before **May 14th 11:59pm**.    

\newpage

# Module 1, 2, and 3
In the questions for Modules 1, 2, and 3, we will look at data from SNCF, France's national railway. The data has been cleaned and made easily available by [TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26). In particular, we have data on train delays from each month between 2015-2018 for each train route (i.e., from city A to city B). So each observation (i.e., row in the data) corresponds to a specific route in a specific year and month. In the dataset, we will be particularly interested in the following variables

For each row in the data, we have the following variables

* year : year of observation (2015, 2016, 2017 or 2018)
* month : month of observation (1, 2, ..., 12)
* departure_station : station where the route begins (e.g., "PARIS NORD" or "MONTPELLIER")
* arrival_station : station where the route ends (e.g., "PARIS NORD" or "MONTPELLIER")
* journey_time_avg : average journey time in minutes for the route for that year and month
* avg_delay_all_departing : average delay in minutes  for all departures for the route for that year and month (i.e., how many minutes the train was late to leave departure station)
* avg_delay_all_arriving : average delay in minutes for all arrivals for the route for that year and month (i.e., how many minutes the train was late to arrive at the arrival_station)

In the following questions, the model you fit or consider may change from question to question.


```{r, fig.align='center', fig.height=3}
## Load in data and remove some outliers
train_data <- read.csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv")
# removing some outliers of long train delays
train_data <- train_data[-which(train_data$avg_delay_all_arriving < -30),]
train_data <- train_data[-which(train_data$avg_delay_all_departing > 30),]
# make month and year factors
train_data$month <- as.factor(train_data$month)
train_data$year <- as.factor(train_data$year)
```


## Question 1 (2 pts)
Suppose we are interested in modeling the average delayed arrival; i.e., avg_delay_all_arriving is the outcome variable. Specifically, we would like to investigate the association between average delayed arrival and journey time (journey_time_avg) when controlling for the average departure delay (avg_delay_all_departing).

Fit the relevant linear model below and write 1 sentence interpreting the estimated coefficient for journey_time_avg. 

#### Question 1 Answer

```{r}
#The p-value for journey_time_avg tests whether journey time is significantly associated with average delayed arrival, after controlling for the effect of avg_delay_all_departing. The journey_time_avg p-value reflects the unique contribution of journey time to explaining arrival delay.
delay_a <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing, data = train_data)
summary(delay_a)

```


## Question 2 (2 pts)
Some output for a **different model** is shown below. Using the output, predict the average arrival delay for a train route which has an average journey time of 200 minutes, has an average departure delay of 3 minutes, and took place in January (i.e., month == 1). 
```{r, echo =F}
#intercept = month1
#y_hat = Beta0 + Beta1*X + Beta2*X
#avg_delay_all_arriving = Beta0 + Beta1*journey_time_avg + Beta2*avg_delay_all_departing
mod2 <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing + month,
           data = train_data)
summary(mod2)$coef
```
#### Question 2 Answer

The equation to find the average arrival delay for a particular train route is **avg_arrival_delay = Beta0 + Beta1*journey_time_avg + Beta2*avg_delay_all_departing**. 

Beta0 = intercept estimate = -0.89153617
Beta1 = journey_time_avg coefficient = 0.02215535
Beta2 = avg_delay_all_departing coefficient = 0.79854766
journey_time_avg = 200 min
avg_delay_departing = 3 min

avg_arrival_delay = -0.89153617 + 0.02215535x200 + 0.79854766x3
= **5.935 minutes**


## Question 3 (6 pts)
Do the assumptions for linear regression seem reasonable for the model fit in Question 2? Explain why or why not? You should use the plots below to justify your answer.
```{r, fig.align='center', fig.height=3, echo = F}
#fitted values - values predicted by the model
#residual values - differences between observed and predicted values
#a cluster around 0 when the two are plotted against each other shows that the difference between fitted values and observed values is the same no matter the fitted value. If the residuals trend larger or smaller for some range of fitted values, it means there is a structure that is unaccounted for in the model.
#the observed values vs fitted values plot shows that the predicted values are similar to the observed values
par(mfrow = c(1,2))
plot(mod2$fitted.values, mod2$residuals, pch = 19, cex = .1,
     xlab = "fitted values", ylab = "residuals")
plot(mod2$fitted.values, train_data$avg_delay_all_arriving,
     pch = 19, cex = .1 , xlab = "fitted values", ylab = "observed values")
abline(a = 0, b = 1, col = "red")
```


#### Question 3 Answer

One of the assumptions of a linear model is, of course, linearity. In this case it looks like the residuals are centered around 0 as per the fitted values plots, so it looks like the relationship between the variables is linear. A linear model also assumes constant variance, and it looks like the variance across residuals is mostly constant. The residuals must also be independent of each other, and since there is no curve visible in the residuals vs fitted values plot, this assumption seems to hold true. Because the points in the observed vs fitted values seem to be clustered around the line of best fit.



## Question 4 (2 pts)
Suppose you think the association between arrival delay and journey time (i.e., the slope of journey time) may change from year to year. Fit a linear model below which would allow for that. For this problem, you **do not** need to consider adjusting for other variables in the model.

```{r}
#add "year" to the model as a variable
mod_jt <- lm(avg_delay_all_arriving ~ journey_time_avg,
           data = train_data)
summary(mod_jt)

mod_year <- lm(avg_delay_all_arriving ~ journey_time_avg + year,
           data = train_data)
summary(mod_year)

mod_year_int <- lm(avg_delay_all_arriving ~ journey_time_avg*year,
           data = train_data)
summary(mod_year_int)

```

### Question 4 Answer

Model without interaction term, Adjusted R-squared:  0.1952
Model with year, Adjusted R-squared:  0.2758 
Model with year interaction term, Adjusted R-squared:  0.2762

The models which include year explain more of the variance in the dataset and thus have higher adjusted R-squared values. The model with the interaction term doesn't explain much more variance than the model with year alone, so it is likely that the effect of year on journey_time_avg is not significant.



## Question 5 (3 pts)
Below, we fit a model which includes the covariates journey time, average departing delay and month. Suppose we want to test if the average arrival delay is associated with month after adjusting for journey time and average departure delay. For this problem, you don't need to consider interaction terms and you don't need to include other covariates. Describe how you would test this hypothesis. You don’t need to actually perform any calculations or write any code, but specify which function in R you would use and be specific about what the inputs would be.
 
```{r}
mod_year <- lm(avg_delay_all_arriving ~ journey_time_avg + avg_delay_all_departing + month,
               data = train_data)
summary(mod_year)
```
#### Question 5 answer

I would use the anova() function in R to compare two linear models: A reduced model that includes only journey_time_avg and avg_delay_all_departing and a full model that includes journey_time_avg, avg_delay_all_departing, and month. This would show if improving month significantly improves model fit, meaning it has a significant effect on average arrival delay. I could also run an ANOVA on just the model given to see if the month covariate is significant after accounting for journey time average and average delay in departure. 

## Question 6 (2 pt)
Suppose we fit the model below where we have used the log of journey_time_avg. Write 1 sentence interpreting the coefficient for journey time.  

```{r}
mod_log <- lm(avg_delay_all_arriving ~ log(journey_time_avg),
              data = train_data)
summary(mod_log)
```
#### Question 6 answer
Based on the coefficient of log(journey_time_avg), a 1% increase in journey time is associated with an estimated increase of an average of 0.033 minutes.

average arrival delay = -11.065 + 3.29684*log(journey_time_avg)




# Module 4 and 5


## Question 7 (3 pts)
In the model you fit in Question 1, each observation in the dataset corresponds to a specific route observed in a specific month and year. Thus each route appears in the data multiple times. Explain why this might violate an assumption for linear regression. How could you fix this? If your suggestion involves additional covariates or a different modeling assumption, be specific about what you mean (i.e., say what covariates would you include, or what model you would fit). There is more than 1 reasonable answer for this question, but just pick one.

#### Question 7 answer

It violates the assumption of independence. All of the data points from the same route will be correlated with each other because the arrival of the train on a given route may on the arrival time of the train that went down that route earlier, especially if it is the same train. Additionally, every train on the same route is subject to the same weather, infrastructure, and operators, all of which may influence delay on that particular route but not anywhere else. Adding route to the model as a random effect would give each route its own baseline delay level which would account for the non-independence of data points on the same route. 


## Question 8 (3 pts)
Using the model from Question 5, we plot the fitted values vs the residuals below. Explain why you might want to use robust standard errors. What might be the advantages and disadvantages of using the robust standard errors as opposed to the model based errors (the ones that come out of \texttt{summary})?

```{r, echo = F}
mod_log <- lm(avg_delay_all_arriving ~ log(journey_time_avg),
              data = train_data)
plot(mod_log$fitted.values, mod_log$residuals, pch = 19, cex = .1)
```

#### Question 8 answer

There is some heteroscedasticity in this plot that I didn't really pick up in the earlier plots. It looks like the fitted values trend towards higher residuals the higher the fitted value, which suggests there is some population structure information unaccounted for in the model. 

Robust standard errors could account for the heteroscedasticity since they recalculate the variability of the regression coefficients in a way that does not rely on the errors having equal variance. This would be beneficial to the predictive ability of the model. On the other hand, robust standard errors may still lead to unreliable predections depending on the severity of the heteroscedasticity.



## Question 9 (3 pts)
Suppose you are taking a train tomorrow from Lille to Paris Nord and want to predict the delay in arrival. You want to be very sure about the prediction, so you gather data for 1000 different variables you think might be relevant (temperature, whether it is raining, GDP of France per month/year, the win/loss record of the soccer team in Lille, etc). You then regress average arrival delay onto all of those variables, and use it to predict the arrival delay for tomorrow's train. Explain why this might not give a good prediction. What might you do instead? 2-3 sentences for this answer is fine.

#### Question 9 answer

Using many different variables can lead to overfitting, which is when the model treats random noise as predictive. When a model is overfitted, it works very well for the dataset it is trained on, but has little predictive power for new data. Additionally, a model with a thousand variables will be very computationally complex with little benefit to model accuracy to show for it. Instead of using all 1000 variables, I would look at the portion of the variance explained by each coefficients and only keep the coefficients which explain a significant portion of variance. I would test a few different models with this pared down selection of coefficients and compare the models using R-squared, AIC, and BIC.



# Module 6
For the following questions, suppose we are analyzing data for Big Red Airlines, Cornell's latest idea for getting people to and from Ithaca. The dependent variable is whether or not a flight took off on time. In the \texttt{OnTime} variable: 1 indicates that the flight took off on time, 0 indicates that it was delayed. The covariates we have recorded include Temperature (in degrees), TimeOfDay (Evening, Midday, Morning), and Rain (FALSE, TRUE). 
```{r}
airlineData <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lab11/airline.csv")
names(airlineData)
```

## Question 10 (2 pts)
What is the appropriate type of regression for modeling the binary data? What is being predicted by the linear model we are fitting? i.e., if the model we set up is 
$$ ? = b_0 + b_1 X_{1,i} + b_2 X_{2,i} \ldots$$ what is on the left side of the equation (you can write it out in words instead of typing out the math)?. 

#### Question 10 answer

The best type of regression for this binary data is a logistic model, in which case the left side of the equation would be the variable "log odds of flight taking off on time."


## Question 11 (2 pts)
We fit the model below. How would you interpret the coefficient associated with \texttt{Temperature}?
```{r}
mod <- glm(OnTime ~ Temperature + TimeOfDay + Rain,
           data = airlineData, family = "binomial")
summary(mod)
```
#### Question 11 answer

Temperature: -0.05248

The coefficient represents the change in the log-odds of a flight taking off on time for each one-degree increase in temperature (when TimeOfDay and Rain are constant, since they are also included in the model). For each one degree increase in temperature, the log odds of the flight taking off on time decrease by 0.05248.



\newpage

# Final Project (30 pts)

## Introduction

This final project is designed to demonstrate your mastery of linear regression techniques on real-world data. You will apply the theoretical concepts we've covered in class to a dataset of your choice, perform a comprehensive analysis, and present your findings in a professional format suitable for showcasing to potential employers.

## Objectives

By completing this project, you will:

* Apply linear regression techniques to solve real-world problems
* Demonstrate your ability to verify and address regression assumptions
* Perform meaningful feature selection and hypothesis testing
* Communicate the practical significance of your statistical findings
* Create a professional portfolio piece for future employment opportunities

## Project Requirements

### Dataset Selection

1. Choose a dataset from Kaggle
2. Your dataset must have a continuous target variable suitable for linear regression
3. The dataset should contain multiple potential predictor variables
4. Choose a dataset that interests you and has meaningful real-world applications

Dataset selected: https://www.kaggle.com/datasets/rtatman/188-million-us-wildfires

```{r}
#Download dataset and convert to csv
#install.packages("DBI")
#install.packages("RSQLite")

library(DBI)
library(RSQLite)
library(tidyverse)

con <- dbConnect(SQLite(), "FPA_FOD_20170508.sqlite")
dbListTables(con)
US_wildfires <- dbGetQuery(con, "
  SELECT FOD_ID, FIRE_YEAR, DISCOVERY_DATE, DISCOVERY_DOY, DISCOVERY_TIME, STAT_CAUSE_CODE, STAT_CAUSE_DESCR, CONT_DATE, CONT_DOY, CONT_TIME, FIRE_SIZE, FIRE_SIZE_CLASS , LATITUDE, LONGITUDE, OWNER_CODE, OWNER_DESCR, STATE FROM Fires
")
dbDisconnect(con)
```


### Analysis Requirements
Your analysis must include the following components:

#### Exploratory Data Analysis

* Summary statistics of variables
```{r}
US_wildfires$DISCOVERY_TIME <- sprintf("%04s", US_wildfires$DISCOVERY_TIME)
#Convert to hours 
US_wildfires$DISCOVERY_HOUR <- as.numeric(substr(US_wildfires$DISCOVERY_TIME, 1, 2)) +
                                as.numeric(substr(US_wildfires$DISCOVERY_TIME, 3, 4)) / 60

US_wildfires <- US_wildfires %>%
  filter(!is.na(DISCOVERY_HOUR))



US_wildfires$CONT_TIME <- ifelse(
  grepl("^\\d{3,4}$", US_wildfires$CONT_TIME),
  sprintf("%04s", US_wildfires$CONT_TIME),  # pad with 0s like "0800"
  NA
)

US_wildfires$CONT_HOUR <- as.numeric(substr(US_wildfires$CONT_TIME, 1, 2)) +
                          as.numeric(substr(US_wildfires$CONT_TIME, 3, 4)) / 60



str(US_wildfires)
summary(US_wildfires)
#FIRE_YEAR goes from 1992 to 2015

```
```{r}
table(US_wildfires$FIRE_SIZE_CLASS)
table(US_wildfires$STAT_CAUSE_DESCR)
table(US_wildfires$OWNER_DESCR)
table(US_wildfires$STATE)
```


* Visualization of distributions and relationships

```{r}
#Distribution of fire sizes in log scale
ggplot(US_wildfires, aes(x = FIRE_SIZE)) +
  geom_histogram(bins = 100) +
  scale_x_log10() +
  labs(title = "Distribution of Fire Size (log scale)", x = "Fire Size (Acres)", y = "Count")

```
```{r}
#Time of day
ggplot(US_wildfires, aes(x = DISCOVERY_HOUR)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Discovery Time of Fires", x = "Hour of Day", y = "Count")
```

```{r}
#Fire size by cause
ggplot(US_wildfires, aes(x = reorder(STAT_CAUSE_DESCR, FIRE_SIZE, median, na.rm = TRUE), y = FIRE_SIZE)) +
  geom_boxplot(outlier.shape = NA, fill = "lightblue") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Fire Size by Cause", x = "Cause", y = "Fire Size (log scale)")
```
```{r}
#Fire discovery time
ggplot(US_wildfires, aes(x = DISCOVERY_HOUR)) +
  geom_histogram(binwidth = 1, fill = "darkorange", color = "black") +
  labs(title = "Time of Day Fires Were Discovered", x = "Hour", y = "Count")
```
```{r}
#Fire Size by time of day
ggplot(US_wildfires, aes(x = DISCOVERY_HOUR, y = FIRE_SIZE)) +
  geom_point(alpha = 0.1) +
  scale_y_log10() +
  labs(title = "Fire Size by Time of Day", x = "Discovery Hour", y = "Fire Size (log scale)")

```


```{r}
#fire size by day of year
ggplot(US_wildfires, aes(x = DISCOVERY_DOY, y = FIRE_SIZE)) +
  geom_point(alpha = 0.1) +
  scale_y_log10() +
  labs(title = "Fire Size by Day of Year", x = "Day of Year", y = "Fire Size (log scale)")
```

* Identification of missing values and outliers

```{r}
#find missing values
colSums(is.na(US_wildfires))

```
```{r}
#find outliers
summary(US_wildfires$FIRE_SIZE)
quantile(US_wildfires$FIRE_SIZE, probs = c(0.95, 0.99, 0.999))
```

```{r}
#view number of outliers
outlier_threshold <- quantile(US_wildfires$FIRE_SIZE, 0.999, na.rm = TRUE)

sum(US_wildfires$FIRE_SIZE > outlier_threshold)
```

* Data cleaning and preprocessing steps

```{r}
US_wildfires <- US_wildfires %>% filter(!is.na(CONT_HOUR))
US_wildfires <- US_wildfires %>% filter(!is.na(CONT_DATE))

colSums(is.na(US_wildfires))
```


#### Regression Assumptions Verification

* Linearity assessment

```{r}
#Log fire size will be used to account for there being a small number of very large fires
#add log fire size to dataset
US_wildfires <- US_wildfires %>%
  mutate(LOG_FIRE_SIZE = log(FIRE_SIZE + 1))

```


```{r}
#Determine if the model fit is linear using a few coefficients from the dataset
model1 <- lm(LOG_FIRE_SIZE ~ DISCOVERY_DOY + FIRE_YEAR, data = US_wildfires)
summary(model1)

```


```{r}
#The linear model doesn't fully capture the relationship between predictors and the log-transformed fire size. Interaction terms and non-linear transformations are probably needed
plot(model1$fitted.values, model1$residuals,
     pch = 19, col = rgb(0, 0, 0, 0.3),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lwd = 2)

```


* Normality of residuals
* Homoscedasticity (constant variance of residuals)

```{r}
#The Q-Q plot has a right skew and heavy tail which indicates that the residuals are not normally distributed. The Q-Q plot shows that they are very closely clustered around 0.
#Residuals show increasing variance with fitted values, which violates the homoscedasticity assumption. This could bias standard errors.

#Histogram of residuals
hist(model1$residuals, breaks = 100, main = "Histogram of Residuals", xlab = "Residuals")

#Q-Q plot
qqnorm(model1$residuals)
qqline(model1$residuals, col = "red", lwd = 2)

```

* Independence of observations
```{r}
#Check for autocorrelation
#Independence assumption is satisfied because it seems there is no autocorrelation
library(car)

set.seed(123)
subset_rows <- sample(nrow(US_wildfires), 10000)
US_sample <- US_wildfires[subset_rows, ]

model_small <- lm(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + STAT_CAUSE_DESCR + FIRE_YEAR + LATITUDE + LONGITUDE,
                  data = US_sample)

durbinWatsonTest(model_small)

```


* Multicollinearity assessment

```{r}
#no multicolinearity between the chosen predictors
vif(model1)

```



#### Assumption Violation Handling

* Apply appropriate transformations when assumptions are violated
* Document your approach to each violation
* Compare models before and after corrections

the model violates linearity, normality, and homoscedasticity, so that is what will need to be handled.

```{r}
#this model fit is more highly predictive than the simple model, although it still has very little predictive power
model_poly <- lm(LOG_FIRE_SIZE ~ poly(DISCOVERY_HOUR, 2) +
                                  poly(DISCOVERY_DOY, 2) +
                                  STAT_CAUSE_DESCR +
                                  LATITUDE + LONGITUDE +
                                  FIRE_YEAR,
                 data = US_wildfires)

summary(model_poly)
```
```{r}
#weighted least squares to reduce heteroscedasticity

library(nlme)

model_wls <- gls(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + STAT_CAUSE_DESCR +
                                  FIRE_YEAR + LATITUDE + LONGITUDE,
                 weights = varExp(),  #allows for increasing variance
                 data = US_wildfires)

summary(model_wls)
```
```{r}
#compare models
AIC(model, model_wls, model_poly)

```

```{r}
#refit models to make sure they use the same number of observations
model_data <- US_wildfires %>%
  select(LOG_FIRE_SIZE, DISCOVERY_HOUR, DISCOVERY_DOY,
         STAT_CAUSE_DESCR, LATITUDE, LONGITUDE, FIRE_YEAR) %>%
  drop_na()

#OLS model
model_lm <- lm(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + STAT_CAUSE_DESCR +
                                 FIRE_YEAR + LATITUDE + LONGITUDE,
               data = model_data)

#Poly model
model_poly <- lm(LOG_FIRE_SIZE ~ poly(DISCOVERY_HOUR, 2) +
                                   poly(DISCOVERY_DOY, 2) +
                                   STAT_CAUSE_DESCR + LATITUDE + LONGITUDE +
                                   FIRE_YEAR,
                 data = model_data)

#WLS model
library(nlme)
model_wls <- gls(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + STAT_CAUSE_DESCR +
                                   FIRE_YEAR + LATITUDE + LONGITUDE,
                 weights = varExp(),
                 data = model_data)
```
```{r}
AIC(model_lm, model_poly, model_wls)
#poly model has the lowest AIC
```


After checking assumptions, nonlinearity and heteroscedasticity in the residuals of the initial model. To address this, we applied a second-degree polynomial transformation to key predictors. The revised model showed improved residual behavior and lower AIC, indicating better fit.


#### Variable Selection & Hypothesis Testing

* Implement at least two different variable selection techniques
* Perform hypothesis tests on coefficients
* Assess model performance with metrics (R², adjusted R², RMSE, etc.)

```{r}
#backward elimination: start with all the variables and remove the weakest
model_full <- lm(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + DISCOVERY_DOY + 
                                   FIRE_YEAR + LATITUDE + LONGITUDE +
                                   STAT_CAUSE_DESCR,
                 data = US_wildfires)

model_back <- step(model_full, direction = "backward")
summary(model_back)

#DISCOVERY_HOUR: Later discoveries significantly correlate with smaller fires
#DISCOVERY_DOY: Later dates significantly correlate with smaller fires
#FIRE_YEAR: Small positive trend - fire size slightly increasing over years
#LATITUDE: Fires get smaller moving north
#LONGITUDE: Fires get smaller moving east
#STAT_CAUSE_DESCR: Many causes significantly decrease fire size (Smoking, Lightning, Debris Burning)

#many aspects of the model are significant but it still has a very low R-squared
```
```{r}
#forward selection - add one variable at a time based on AIC and BIC
model_null <- lm(LOG_FIRE_SIZE ~ 1, data = US_wildfires)

model_fwd <- step(model_null,
                  scope = formula(model_full),
                  direction = "forward")
summary(model_fwd)

#forward and backward selection led to the same model, which is a good sign for those predictors. We will use this as the final model.

```
Final Model: LOG_FIRE_SIZE ~ STAT_CAUSE_DESCR + DISCOVERY_DOY + LATITUDE + LONGITUDE + DISCOVERY_HOUR + FIRE_YEAR



* Validate your model using appropriate cross-validation techniques

```{r}
#install.packages("caret")
library(caret)

set.seed(123)
train_control <- trainControl(method = "cv", number = 10)

model_cv <- train(
  LOG_FIRE_SIZE ~ STAT_CAUSE_DESCR + DISCOVERY_DOY + LATITUDE +
                  LONGITUDE + DISCOVERY_HOUR + FIRE_YEAR,
  data = US_wildfires,
  method = "lm",
  trControl = train_control
)

model_cv$results
```
Interpretation
RMSE - On average, predictions are off by ~1.42 log units
R^2 -  3.7% of variance in log-fire size is explained
MAE -	Median absolute prediction error = 0.97 log units
RMSE SD	- Very stable error across folds
R^2 SD	- Cross-validated R^2 is consistent across folds


#### Feature Impact Analysis

* Quantify and interpret the impact of each feature on the target
* Provide confidence intervals for significant coefficients

```{r}
model_final <- lm(LOG_FIRE_SIZE ~ DISCOVERY_HOUR + DISCOVERY_DOY + 
                                   FIRE_YEAR + LATITUDE + LONGITUDE +
                                   STAT_CAUSE_DESCR,
                 data = US_wildfires)

confint(model_final)
```



* Explain the practical significance of your findings in the context of the dataset


#### Deliverables

GitHub Repository containing:

* All code (well-documented Rmd files)
* README.md with clear instructions on how to run your analysis
* Data folder (or instructions for accessing the data)
* Requirements.txt or environment.yml file


#### Final Report (PDF) containing:

* Introduction: dataset description and problem statement
* Methodology: techniques used and justification
* Results: findings from your analysis
* Discussion: interpretation of results and limitations
* Conclusion: summary and potential future work
* References: cite all sources used



## Evaluation Criteria
Your project will be evaluated based on:

* Correctness of statistical analysis and procedures
* Proper handling of regression assumptions
* Quality of variable selection and hypothesis testing
* Clarity of interpretation and insights
* Organization and documentation of code
* Professional presentation of findings

## Timeline and Submission

* Release Date: May 5th, 2025
* Due Date: Wednesday, May 14th, 2025 (11:59 PM EST)
* Submission: Email your GitHub repository link and PDF report to nbb45@cornell.edu with the subject line "Final Project - [Your Name]"

## Resources

* Course materials and lecture notes
* [Kaggle Datasets](https://www.kaggle.com/datasets)
* [GitHub tutorial](https://nayelbettache.github.io/documents/STSCI_6020/Github_tutorial.pdf) and [GitHub documentation](https://docs.github.com/en/repositories) for repository setup.

## Academic Integrity
This is an individual project. While you may discuss general concepts with classmates, all submitted work must be your own. Proper citation is required for any external resources used.

Good luck with your project! This is an opportunity to demonstrate your skills and create a valuable addition to your professional portfolio.

# Finished


You're done, congratulations!

